# 自己搭建简单的神经网络
## 简单总结
最近想了想自己未来的发展，并做了简单的规划，总结了常见的研究领域和方向，认为自己最擅长的还是数学和编程，故此比较适合人工智能方向，而人工智能最好入手的学习方法就是深度学习，所以听了听李宏毅老师的课，对机器学习有一个初步的认识，本次写博客的目的是记录pytorch的简单使用方法，以及对神经网络有一个初步的认识。
## 1. 我对神经网络的理解
神经网络比较抽象，从字面意思理解，貌似是模拟人体神经元这类高大上的东西，实际二者只是长得相似，事实却并非如此。
机器学习的目的是为了找到一个函数，可以实现很复杂的功能。比如将一张图片放入函数中，即可输出图片包含的内容，又或者将一段录音放入函数中，即可输出录音内的汉字。
这个函数在某种意义上非常复杂，没有办法用常规的模型对函数建模。解决方法是赋予函数控制参数变化的能力，使得函数可以通过对比预测值和输出值，自行拟合出想要的函数，这就是学习过程。
## 2. 简单神经网络的步骤
搭建简单的神经网络我个人分为如下几个步骤：
1. 输入一个一维特征数组:  `X`
2. 左乘一个二维参数数组:  `WX`
3. 加上一个二维参数数组:  `B+WX`
4. 模型出来后，就是这样：`y = B+WX`
5. 写出损失函数:`Loss = ŷ - y`
6. 计算损失函数的微分(θ代表的是W和B)：`d(L)/d(θ)`
7. 梯度下降,求损失函数的极小值：`y = (B-b) + (W-w) X`
8. 需要的话加入一个激活函数，使得非线性拟合能力增强

Ok大功告成。
## 3.Kaggle简单的线性拟合练习：
[作业链接](https://www.kaggle.com/c/ml2021spring-hw1/submit ),
这个就是简单的一次搭建，做了一个4层的神经网络，(其实层不层的没什么感觉，就改个数)。得分是
![image](https://user-images.githubusercontent.com/38205098/132837965-21326219-02fd-40ac-bcac-611d7747798c.png)
讲实在的水平还是不够，真不知道该怎么调参，仅仅过了一个simple baseline而已。
看了看medium baseline的介绍，发现自己很多都看不懂，革命尚未成功，同志仍需努力啊！
```python
import numpy as np
tmp = np.loadtxt('./covid.train.csv', delimiter=",", dtype="str")
tmp1 = np.loadtxt('./covid.test.csv', delimiter=",", dtype="str")
featrue = tmp[1:,1:94].astype(np.float32)
label = tmp[1:,94:95].astype(np.float32)
testdata = tmp1[1:,1:94].astype(np.float32)

import torch
from torch import nn


class OneNeuralNetwork(nn.Module):
    def __init__(self):
        super(OneNeuralNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(93, 100),
            nn.ReLU(),
            nn.Linear(100, 80),
            nn.ReLU(),
            nn.Linear(80, 10),
            nn.Linear(10, 1),
            nn.ReLU(),
        )
    def forward(self, x):
        logits = self.model(x)
        logits = logits.squeeze(-1)
        return logits
NetWork = OneNeuralNetwork()

loss_func = nn.MSELoss()


optimizer = torch.optim.SGD(NetWork.parameters(), lr=1e-5)
print("id,tested_positive")
def train():
    for i in range(len(featrue)):
        result = NetWork(torch.from_numpy(featrue[i]))
        loss = loss_func(result,torch.tensor(torch.from_numpy(label[i]),dtype=torch.float32))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
def test():
    for i in range(len(testdata)):
        result = NetWork(torch.from_numpy(testdata[i]))
        print(f"{i},{result}")
for i in range(85):
    train()
test()
```
